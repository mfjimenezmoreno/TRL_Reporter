{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ed6051",
   "metadata": {},
   "source": [
    "# Testing Agentic approach to rubric evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74027ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import glob, os, time, json, re, http, asyncio\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from agents import Agent, Runner, Tool, trace\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "# Escape Codes para imprimir mensajes con color\n",
    "Rd = \"\\033[1;31m\"\n",
    "Grn = \"\\033[1;32m\"\n",
    "Ylw = \"\\033[1;33m\"\n",
    "Blu = \"\\033[1;34m\"\n",
    "Mag = \"\\033[1;35m\"\n",
    "Rst = \"\\033[0m\"\n",
    "\n",
    "spacer = f\"{80*'-'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1df4556",
   "metadata": {},
   "source": [
    "Rubric Evaluation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90389c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reference(str, Enum):\n",
    "    Robust = \"Robust\"\n",
    "    Weak = \"Weak\"\n",
    "    Not_Found = \"Not Found\"\n",
    "    Evidence_Not_Needed = \"Evidence is not needed: Trivial\"\n",
    "\n",
    "class Evidence(BaseModel):\n",
    "    \"\"\"Formato de evidencia, que indica si es claro y sustentado con evidencia.\"\"\"\n",
    "    evidence: str = Field(description=\"Extracto del texto que justifica el criterio.\")\n",
    "    clarity: str = Field(description=\"Indica que la evidencia es clara y coherente o si le falta claridad.\")\n",
    "    referencia: Reference = Field(description=\"Referencia o sustento científico de la evidencia. Si el enunciado es algo genérico o trivial, regresar que no se necesita. Si la evidencia es de tipo muy fuerte y no tiene referencia, regresar no encontrado. Si la evidencia es débil, regresar que es débil. Si se encuentra la referencia, regresar que no es robusta.\")\n",
    "\n",
    "class TechnicalMeritRubicCriterion(BaseModel):\n",
    "    \"\"\"Rúbrica de Criterios para evaluar el mérito técnico del proyecto.\"\"\"\n",
    "    \n",
    "    technical_evidence: List[str] = Field(description=\"Evidencias del criterio.\")\n",
    "    scientific_justification: List[str] = Field(description=\"Justificación científica del criterio.\")\n",
    "    \n",
    "\n",
    "class MaturityRubric(BaseModel):\n",
    "    \"\"\"Formato de Rúbrica, para juntar las evidencias relacionadas a la madurez de un proyecto científico.\"\"\"\n",
    "    merito_tecnico: List[TechnicalMeritRubicCriterion] = Field(description=\"Evidencias relacionadas a la base técnica del proyecto y su justificación científica.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_evaluator_agent = Agent(\n",
    "    name=\"Rubric Evaluator Agent\",\n",
    "    description=\"\"\"\n",
    "    Description:\n",
    "    The Rubric Evaluator Agent is responsible for performing an initial evaluation of the proposal document against a maturity assessment rubric. This agent extracts relevant evidence and information from the document, aligning it with predefined rubric criteria. The purpose is to provide a structured, criteria-aligned summary that highlights what is present, what is missing, and what is ambiguous in terms of maturity evidence.\n",
    "\n",
    "    Inputs:\n",
    "    1. The proposal or project document containing technical, methodological, and contextual information.\n",
    "    2. A rubric detailing maturity assessment criteria across multiple fields (e.g., technology readiness, reproducibility, validation, scalability, integration, impact).\n",
    "\n",
    "    Objective:\n",
    "    1. Parse the proposal document and extract supporting evidence for each rubric item.\n",
    "    2. Classify each rubric item as one of the following:\n",
    "        - Met: clear and relevant evidence is found.\n",
    "        - Not Met: no evidence is present or the claim is unsupported.\n",
    "        - Partial: some evidence exists but is incomplete or unclear.\n",
    "    3. Output a structured report (e.g., JSON or table) summarizing rubric fulfillment status, citing evidence where applicable.\n",
    "\n",
    "    Adjust to Feedback:\n",
    "    1. If additional documentation or clarifications are provided, the agent can re-process and update the rubric assessment.\n",
    "    2. Responds to requests from other agents during the main discussion phase by clarifying where and how evidence supports rubric items.\n",
    "\n",
    "    Note:\n",
    "    This agent operates prior to the debate among the other agents (e.g., Proponent, Devil’s Advocate) and provides an evidence-based baseline that frames the ensuing discussion.\n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    memory=None,\n",
    "    verbose=True,\n",
    "    temperature=0.3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22389e76",
   "metadata": {},
   "source": [
    "Multi Agent Argumentation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6eba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "proponent_agent = Agent(\n",
    "    name=\"Proponent Agent\",\n",
    "    description=\"\"\"\n",
    "    Description:\n",
    "    The Proponent Agent presents the proposal or document for review, providing the strongest rationale and evidence in favor of the proposed project or concept. This agent is tasked with making a compelling case for why the proposal should be accepted.\n",
    "\n",
    "    Inputs:\n",
    "    1. The document containing the proposal, evidence, and background information.\n",
    "\n",
    "    2. Any supporting data, previous success stories, or detailed methodologies used in the proposal.\n",
    "\n",
    "    Objective:\n",
    "    1. Present the proposal in a clear and persuasive manner, highlighting key strengths and justifications for its implementation.\n",
    "    2. Respond to critiques from other agents during the review process.\n",
    "\n",
    "    Adjust to Feedback:\n",
    "    1. Incorporate feedback from the critique rounds to strengthen areas of the proposal that were found to be lacking.\n",
    "    2. Address weaknesses or gaps in evidence, clarifying and providing additional supporting materials where necessary.\n",
    "    \n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    memory=None,\n",
    "    verbose=True,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "devils_advocate_agent = Agent(\n",
    "    name=\"Devil's Advocate Agent\",\n",
    "    description=\"\"\"\n",
    "    Description:\n",
    "    The Devil’s Advocate Agent identifies potential weak points, risks, and inconsistencies in the proposal. Their role is to challenge the assumptions and conclusions made in the document, uncovering areas that might lead to failure or unintended negative outcomes.\n",
    "\n",
    "    Inputs:\n",
    "    1. The proposal document.\n",
    "    2. Rubric highlighting areas of concern or weaknesses in the evidence.\n",
    "\n",
    "    Objective:\n",
    "    1. Find flaws or inconsistencies within the proposal, pointing out where the reasoning is not robust or where assumptions may be unwarranted.\n",
    "    2. Introduce alternative viewpoints or counterarguments to test the proposal’s validity.\n",
    "\n",
    "    Adjust to Feedback:\n",
    "    1. Reflect on the responses to their critiques during the Cross-Critique phase. If their challenges are found to be weak or unjustified, adjust arguments or refine the critique to make it more relevant.\n",
    "    2. Reassess the proposal and its defense after incorporating the Proponent's responses.\n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    memory=None,\n",
    "    verbose=True,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "evidence_reviewer_agent = Agent(\n",
    "    name=\"Evidence Reviewer Agent\",\n",
    "    description=\"\"\" \n",
    "    Description:\n",
    "    The Evidence Reviewer Agent critically examines the evidence provided in support of the proposal. This agent focuses on verifying the quality, validity, and relevance of the data and assumptions used to justify the proposal.\n",
    "\n",
    "    Inputs:\n",
    "    1. The proposal document with the presented evidence.\n",
    "    2. Rubric identifying evidence types required (e.g., statistical analysis, expert opinion, case studies, etc.).\n",
    "\n",
    "    Objective:\n",
    "    1. Challenge the assumptions by reviewing the presented evidence and identifying gaps or areas where evidence is insufficient, misleading, or absent.\n",
    "    2. Suggest additional evidence or data sources that may strengthen the proposal.\n",
    "\n",
    "    Adjust to Feedback:\n",
    "    1. If the Proponent can provide more credible or relevant evidence, adjust the critique to reflect new findings.\n",
    "    2. Revise the evaluation criteria based on the Proponent's ability to counter the evidence critique.\n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    memory=None,\n",
    "    verbose=True,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "feasibility_agent = Agent(\n",
    "    name=\"Feasibility Agent\",\n",
    "    description=\"\"\" \n",
    "    Description:\n",
    "    The Feasibility Agent evaluates whether the proposal is practically executable within the given context. This includes assessing the resources, time, technology, and expertise needed to implement the proposal and whether the proposal is viable in real-world conditions.\n",
    "\n",
    "    Inputs:\n",
    "    1. The proposal document and context information (budget, timeline, resource constraints).\n",
    "    2. Rubric identifying feasibility requirements (e.g., required technology, skillsets, timeframe, etc.).\n",
    "\n",
    "    Objective:\n",
    "    1. Assess the proposal’s practicality and implementation challenges, such as cost, resource constraints, timeline, or technological feasibility.\n",
    "    2. Identify potential barriers to successful implementation.\n",
    "\n",
    "    Adjust to Feedback:\n",
    "    1. Reassess the proposal based on the Proponent's explanations of resource allocation or potential adjustments to the project.\n",
    "    2. Refine feasibility concerns if additional supporting information is provided, such as funding or resource guarantees.\n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    memory=None,\n",
    "    verbose=True,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "ethics_impact_agent = Agent(\n",
    "    name=\"Ethics/Impact Agent\",\n",
    "    description=\"\"\" \n",
    "    Description:\n",
    "    The Ethics/Impact Agent evaluates the broader ethical implications and potential societal impacts of the proposal. This agent considers issues like environmental impact, fairness, equity, privacy, safety, and long-term consequences.\n",
    "\n",
    "    Inputs:\n",
    "    1. The proposal document, with particular attention to the ethical considerations and potential risks outlined.\n",
    "    2. Rubric highlighting required ethical and societal considerations.\n",
    "\n",
    "    Objective:\n",
    "    1. Identify any ethical dilemmas, unintended societal consequences, or risks associated with the proposal.\n",
    "    2. Challenge the proposal on moral grounds, proposing alternative approaches where appropriate.\n",
    "\n",
    "    Adjust to Feedback:\n",
    "    1. If the Proponent addresses concerns with mitigation strategies or revised ethical assessments, adjust critiques accordingly.\n",
    "    2. Reevaluate the ethical considerations based on new information or proposed changes to the project.\n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    memory=None,\n",
    "    verbose=True,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "synthesis_judge_agent = Agent(\n",
    "    name=\"Synthesis Judge Agent\",\n",
    "    description=\"\"\" \n",
    "    Description:\n",
    "    The Synthesis Judge Agent is responsible for synthesizing all the arguments, critiques, and defenses and ultimately making a decision regarding the proposal. This agent evaluates all the inputs and provides a final score or decision (pass, fail, defer) based on the quality of the proposal, the strength of the critiques, and the Proponent’s responses.\n",
    "\n",
    "    Inputs:\n",
    "    1. All previous discussions, critiques, and defenses.\n",
    "    2. The proposal document, including all adjustments made during the critique rounds.\n",
    "\n",
    "    Objective:\n",
    "    1. Weigh all arguments fairly, taking into account the strength of evidence, feasibility, ethics, and risks.\n",
    "    2. Provide a final decision based on the quality of the proposal and the validity of the critiques and defenses.\n",
    "\n",
    "    Adjust to Feedback:\n",
    "    1. If new arguments or adjustments are introduced by the Proponent, the Synthesis Judge must incorporate them into their final evaluation.\n",
    "    2. Continually update the decision based on new insights, especially during the Cross-Critique and Defense rounds.\n",
    "    \"\"\",\n",
    "    tools=[],\n",
    "    memory=None,\n",
    "    verbose=True,\n",
    "    temperature=0.5,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rubric_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
